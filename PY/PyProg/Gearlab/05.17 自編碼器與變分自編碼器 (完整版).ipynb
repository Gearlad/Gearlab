{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder and Variational Autoencoder\n",
    "\n",
    "本單元，我們將介紹並帶各位同學實作非監督式學習中的自編碼器及其變形。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 初始準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, FloatSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras functions\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Lambda, concatenate\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose, UpSampling2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras import metrics\n",
    "\n",
    "# Keras dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Keras utilis function\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 讀入 MNIST 數據庫\n",
    "老規矩，開場就先召喚我們的好朋友 - MNIST 手寫數字數據庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 training data with size 28 x 28\n",
      "There are 10000 testing  data with size 28 x 28\n"
     ]
    }
   ],
   "source": [
    "(x_train0, y_train0), (x_test0, y_test0) = mnist.load_data()\n",
    "\n",
    "print(\"There are %d training data with size %d x %d\" %x_train0.shape)\n",
    "print(\"There are %d testing  data with size %d x %d\" %x_test0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "調整資料長相及單位化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train0.reshape(60000, -1)\n",
    "x_test = x_test0.reshape(10000, -1)\n",
    "\n",
    "x_train -= x_train.min()\n",
    "x_train = x_train/x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder，又稱為自編碼器，是一個將資料壓縮再還原的模型，通常由一編碼器及一解碼器所組成。\n",
    "\n",
    "給定一組數據集 $\\mathcal{D}\\subseteq\\mathbb{R}^n$ 以及一個壓縮維度 $m$，其中 $m < n$。\n",
    "\n",
    "數學上來說表示，Autoencoder 由 $\\mbox{Enc}:\\mathbb{R}^n\\to\\mathbb{R}^m$ 和 $\\mbox{Dec}:\\mathbb{R}^m\\to\\mathbb{R}^n$ 所構成：\n",
    "\n",
    "$$ \\mathbb{R}^n \\overset{\\mbox{Enc}}{\\to} \\mathbb{R}^m \\overset{\\mbox{Dec}}{\\to} \\mathbb{R}^n$$\n",
    "\n",
    "$$ x \\overset{\\mbox{Enc}}{\\mapsto} h \\overset{\\mbox{Dec}}{\\mapsto} \\hat{x} $$\n",
    "\n",
    "且對於任意的 $x\\in\\mathcal{D}$，我們希望 $x\\approx\\hat{x} = Dec(h) = Dec\\big(Enc(x)\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$h = \\mbox{Enc}(x)$ 稱之 $x$ 為潛在表示法 (latent representation)，而 $\\hat{x}$ 則是 $x$ 自編碼過後的還原資料。\n",
    "\n",
    "換言之，我們希望原本的資料 $x$，經過函數 $\\mbox{Enc}$ 編碼成維度比較小的資料 $h$，再透過函數 $\\mbox{Dec}$ 還原成 $\\hat{x}$。\n",
    "\n",
    "一般而言，我們會希望 $\\mbox{Enc}$ 和 $\\mbox{Dec}$  看起來有點對稱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**為了視覺化的目的，經常會考慮 $m=2$ 的狀況。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Autoencoder 與手寫辨識資料 MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們考慮具有下列結構的 Autoencoder:\n",
    "\n",
    "<img src=\"autoencoder.png\" alt=\"drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "為了方便，我們將三個變數的符號表示出來：\n",
    "\n",
    "$$ x \\overset{\\mbox{Enc}}{\\mapsto} h \\overset{\\mbox{Dec}}{\\mapsto} \\hat{x} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(784,))\n",
    "\n",
    "enc_1 = Dense(100, activation='sigmoid')\n",
    "enc_2 = Dense(2, activation='sigmoid')\n",
    "\n",
    "h = enc_2(enc_1(x))\n",
    "\n",
    "dec_2 = Dense(100, activation='sigmoid')\n",
    "dec_1 = Dense(784, activation='sigmoid')\n",
    "\n",
    "x_hat = dec_1(dec_2(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               300       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 784)               79184     \n",
      "=================================================================\n",
      "Total params: 158,186\n",
      "Trainable params: 158,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Model(x, x_hat)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一樣的，當模型 compile 之後，便可以進行資料的訓練、預測等等，請有興趣的同學讀入 MNIST 手寫辨識之料後，自行完成這個模型的訓練。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='mse', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.1129\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0703\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0684\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.0679\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0677\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0676\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0675\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.0674\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.0674\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.0674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2a2936a0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train, batch_size=1024, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果覺得訓練太久的話，也可以使用我們準備好的權重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder.load_weights('autoencoder_handwriting_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 視覺化 - 子模型的取出\n",
    "為了視覺化(及其他潛在應用)，我們也會從 Autoencoder 將 Encoder 和 Decoder 分別定義出來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 78,702\n",
      "Trainable params: 78,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Encoder = Model(x, h)\n",
    "Encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0523 15:04:47.449203 4497241536 network.py:1619] Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_8\" was not an Input tensor, it was generated by layer dense_9.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: dense_9/Sigmoid:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_3:0\", shape=(None, 784), dtype=float32) at layer \"input_3\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-381bf7733afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 此為錯誤範例\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;31m# initializing _distribution_strategy here since it is possible to call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# predict on a model without compiling it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m    137\u001b[0m       \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 284\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m                              \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m                              \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m                              str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m           \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_3:0\", shape=(None, 784), dtype=float32) at layer \"input_3\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "# 此為錯誤範例\n",
    "Decoder = Model(h, x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder 是由 $h$ 開始，因此，我們先準備一個與 $h$ 相同大小的 `Input`，並餵進 `dec_2` 及 `dec_1` 中即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_input = Input(shape=(2,))\n",
    "Decoder = Model(h_input, dec_1(dec_2(h_input)))\n",
    "\n",
    "Decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 視覺化 - Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們隨便抽取一張圖，並透過 Encoder 來算出它的 latent 表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(x_train.shape[0])\n",
    "print(\"第 %d 圖的 latent 表示為 %s\" %(idx, Encoder.predict(x_train[idx: idx+1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接者，我們從 $10000$ 筆測試資料隨機挑選 $3000$ 手寫辨識資料，將其轉換成 latent 表示法，並畫在同一平面上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(x_test.shape[0], size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latents = Encoder.predict(x_test[indices])\n",
    "plt.scatter(latents[:, 0], latents[:, 1], c=y_test0[indices], cmap=\"tab10\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 視覺化 - Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們在 $[0, 1]\\times[0, 1]$ 這個單位正方形內均勻取樣 $15*15$ 個點，並將這 $225$ 個平面上的點，透過 Decoder 進行圖片的還原。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "grid_x = np.linspace(0.05, 0.95, n)\n",
    "grid_y = np.linspace(0.05, 0.95, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = Decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[(n-i-1) * digit_size: (n - i) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Variational Autoencoder\n",
    "若每筆資料的 latent 不僅僅是一個**位置**，而是一個分布，且分布的平均值附近都能還原，那我們應該怎麼做呢？\n",
    "\n",
    "在此，我們將介紹 Variational Autoencder，一種當代知名的自編碼器，就具有上述的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此時 $\\mbox{Enc}$ 和 $\\mbox{Dec}$ 並不直接串接，而是會透過一常態抽樣的函數以下面的形式串接。\n",
    "\n",
    "$$ \\mathbb{R}^n \\overset{\\mbox{Enc}}{\\to} \\mathbb{R}^m\\times\\mathbb{R}^m \\overset{\\mbox{Sampling}}{\\to} \\mathbb{R}^m \\overset{\\mbox{Dec}}{\\to} \\mathbb{R}^n$$\n",
    "\n",
    "$$ x \\overset{\\mbox{Enc}}{\\mapsto} (\\mu, \\sigma^2) \\overset{\\mbox{Sampling}}{\\mapsto} h \\overset{\\mbox{Dec}}{\\mapsto} \\hat{x} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們準備建構的 Variational Autoencoder 的結構如下：\n",
    "\n",
    "<img src=\"variational_autoencoder.png\" alt=\"drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此， Encoder 的作用不是將資料表示成 latent，而是將資料表示成常態分配的兩個參數，平均數與變異數。\n",
    "\n",
    "而 Decoder 也不再直接使用 Encoder 的結果，而是將 Encoder 的結果作為常態抽樣的兩個參數來進行。\n",
    "\n",
    "因此，資料經過 Encoder，會得到一適當大小的常態分配之參數，而 Decoder 則使用這組參數進行抽樣。\n",
    "\n",
    "即使是同一筆資料，Decoder 每次接受到的 latent 表示法可能都不一樣 (但會在某個平均數附近)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要注意到的是，由於變異數恆正的特性，我們可以下面是以學習取對數後的變異數 (log-variance)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 VAE 的建立\n",
    "為了避免混淆，我們重新定義所有變數，首先，我們定義 Encoder 上的三個神經網路層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_1 = Dense(100, activation='sigmoid')\n",
    "# enc_2 = Dense(2, activation='sigmoid')\n",
    "\n",
    "enc_mean = Dense(2)\n",
    "enc_log_var = Dense(2)\n",
    "\n",
    "dec_2 = Dense(100, activation='sigmoid')\n",
    "dec_1 = Dense(784, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(784,))\n",
    "enc_x = enc_1(x)\n",
    "\n",
    "z_mean = enc_mean(enc_x)\n",
    "z_log_var = enc_log_var(enc_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義抽樣函數並透過 ``Lambda`` 將其轉換成 Keras layer。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "透過 $X\\sim N(0, 1)\\Rightarrow \\mu+\\sigma X\\sim N(\\mu, \\sigma^2)$ 和 $\\sigma = e^{\\frac{\\log{\\sigma^2}}{2}}$，我們透過以下方式定義抽樣函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(2,), mean=0., stddev=1)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Lambda(sampling, output_shape=(2,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_x = dec_2(z)\n",
    "x_hat = dec_1(dec_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE = Model(x, x_hat)\n",
    "VAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Loss 函數的建立\n",
    "VAE 的 loss 函數，其由來牽扯一些訊息理論 (information theory) 的知識，因此，我們在此直接建立訓練 VAE 時的 loss 函數。\n",
    "\n",
    "若對 VAE 的理論及模型基本設定有興趣的同學，可以參考下列兩篇論文：\n",
    "* Auto-Encoding Variational Bayes: https://arxiv.org/pdf/1312.6114.pdf\n",
    "* Tutorial on Variational Autoencoders: https://arxiv.org/pdf/1606.05908.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同學有興趣可以證明下面關於 KL 散度在兩個常態分配上的性質：若 $p\\sim N(\\mu_1,\\sigma_1^2)$, $q\\sim N(\\mu_2,\\sigma_2^2)$，則 $KL(p, q) = \\log\\dfrac{\\sigma_2}{\\sigma_1} + \\dfrac{\\sigma_1^2+(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}-\\dfrac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_recon):  \n",
    "    recovery_loss = 784 * metrics.binary_crossentropy(x, x_recon)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return recovery_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 訓練 VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE.compile(loss=vae_loss, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE.fit(x_train, x_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE.save_weights('VAE_handwriting_model_weights.h5')\n",
    "VAE.load_weights('VAE_handwriting_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 VAE 的視覺化呈現\n",
    "與視覺化 Autoencoder 時的方式一樣，我們先分別定義出 Encoder 和 Decoder。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mbox{VAE:}~x \\overset{\\mbox{Enc}}{\\mapsto} (\\mu, \\sigma^2) \\overset{\\mbox{Sampling}}{\\mapsto} h \\overset{\\mbox{Dec}}{\\mapsto} \\hat{x} $$\n",
    "$$\\mbox{Encoder:}~x \\overset{\\mbox{Enc}}{\\mapsto} \\mu$$\n",
    "$$\\mbox{Decoder:}~h \\overset{\\mbox{Dec}}{\\mapsto} \\hat{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_Encoder = Model(x, z_mean)\n",
    "\n",
    "VAE_Encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_Decoder = Model(h_input, dec_1(dec_2(h_input)))\n",
    "\n",
    "VAE_Decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我們進行 Encoder 的視覺化呈現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(x_train.shape[0])\n",
    "print(\"第 %d 圖的 latent 表示為 %s\" %(idx, VAE_Encoder.predict(x_train[idx: idx+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.random.randint(x_test.shape[0], size=1000)\n",
    "VAE_latents = VAE_Encoder.predict(x_test[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(VAE_latents[:, 0], VAE_latents[:, 1], c=y_test0[indices], cmap='tab20')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們進行 Decoder 的視覺化呈現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(x):\n",
    "    x -= x.min()\n",
    "    x /= x.max()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x_vae = np.linspace(-4+0.05, 4-0.05, n)\n",
    "grid_y_vae = np.linspace(-4+0.05, 4-0.05, n)\n",
    "VAE_figure = np.zeros((digit_size * n, digit_size * n))\n",
    "for i, yi in enumerate(grid_x_vae):\n",
    "    for j, xi in enumerate(grid_y_vae):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = VAE_Decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        VAE_figure[(n-i-1) * digit_size: (n - i) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = normalized(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(VAE_figure, cmap='Greys')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 與 Autoencoder 的 Encoder 進行視覺化比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(VAE_latents[:, 0], VAE_latents[:, 1], c=y_test0[indices], cmap='tab20')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(latents[:, 0], latents[:, 1], c=y_test0[indices], cmap='tab20')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 動態比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inBetween(t):\n",
    "    data_0 = x_train0[idx_1]\n",
    "    data_1 = x_train0[idx_2]\n",
    "    data_t = (1-t)*x_train0[idx_1] + t*x_train0[idx_2]\n",
    "\n",
    "    mu_0 = VAE_Encoder.predict(x_train[idx_1:idx_1+1]).squeeze()\n",
    "    mu_1 = VAE_Encoder.predict(x_train[idx_2:idx_2+1]).squeeze()\n",
    "    mu_t = (1-t)*mu_0 + t*mu_1\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax1 = plt.subplot(2, 1, 2)\n",
    "    ax1.scatter(mu_0[0], mu_0[1])\n",
    "    ax1.scatter(mu_1[0], mu_1[1])\n",
    "    ax1.scatter(mu_t[0], mu_t[1])\n",
    "\n",
    "    ax2 = plt.subplot(2, 3, 1)\n",
    "    ax2.imshow(data_0, cmap='Greys')\n",
    "\n",
    "    ax3 = plt.subplot(2, 3, 2)\n",
    "    ax3.imshow(data_t, cmap='Greys')\n",
    "\n",
    "    ax4 = plt.subplot(2, 3, 3)\n",
    "    ax4.imshow(data_1, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1, idx_2 = np.random.randint(x_test.shape[0], size=2)\n",
    "data_0 = x_train0[idx_1]\n",
    "data_1 = x_train0[idx_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(inBetween, t=FloatSlider(value=0.5, \n",
    "                                  min=0, \n",
    "                                  max=1.0,\n",
    "                                  step=0.02,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
