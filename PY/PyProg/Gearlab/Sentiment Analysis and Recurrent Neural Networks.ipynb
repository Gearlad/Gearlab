{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB Movie Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 25000, Testing samples: 25000\n"
     ]
    }
   ],
   "source": [
    "詞彙量 = 4096\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 詞彙量)\n",
    "print(f'Training samples: {len(x_train)}, Testing samples: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect both x_train and y_train random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_train at index 5522\n",
      "[1, 14, 365, 16, 93, 11, 4, 655, 3486, 5, 739, 8, 30, 4, 86, 622, 589, 8, 276, 3752, 313, 23, 4065, 57, 22, 310, 7, 4, 667, 16, 126, 3367, 12, 9, 2, 1026, 11, 2, 15, 165, 2895, 17, 2, 8, 257, 85, 38, 15, 12, 62, 28, 77, 55, 254, 8, 2, 12, 180, 4, 667, 16, 4, 64, 589, 34, 2, 8, 97, 6, 1375, 3870, 31, 7, 107, 11, 4, 157, 6, 255, 2, 2, 2, 9, 2841, 34, 41, 2965, 5, 1697, 37, 11, 801, 2, 405, 2, 41, 59, 9, 2, 21, 36, 528, 376, 41, 233, 44, 41, 2, 303, 75, 79, 574, 19, 4, 2, 2688, 2, 2, 5, 27, 322, 761, 2, 2, 2, 2, 2, 9, 260, 35, 3437, 878, 58, 2795, 41, 1955, 113, 5, 4, 2, 3866, 7, 4, 223, 2, 2, 826, 2, 75, 82, 26, 574, 19, 4, 1729, 7, 745, 2, 2, 1432, 11, 269, 8, 1176, 6, 196, 1309, 46, 3519, 2, 420, 2, 1964, 2, 63, 316, 60, 2, 3242, 308, 2, 256, 34, 2, 2, 2, 9, 24, 290, 4, 781, 10, 10, 2, 69, 77, 6, 1164, 2422, 5, 95, 6, 2, 2422, 159, 29, 1040, 1219, 1861, 19, 4, 2, 7, 2339, 420, 11, 2, 2, 2, 610, 3280, 33, 4, 1164, 443, 2, 301, 12, 16, 6, 1995, 11, 2402, 1009, 5, 2, 62, 2, 1425, 15, 650, 29, 272, 33, 4, 2, 17, 2, 2, 18, 823, 11, 1009, 15, 4, 1164, 1201, 497, 8, 2, 29, 3034, 4, 2, 5, 2, 3519, 2, 17, 73, 17, 99, 976, 2, 5, 2, 2, 2, 2, 11, 3752, 313, 11, 117, 2, 29, 3034, 4, 2, 2, 29, 69, 569, 12, 82, 11, 628, 2, 11, 263, 2, 464, 29, 272, 33, 2, 5, 2, 11, 4, 736, 7, 2, 2, 29, 684, 16, 170, 8, 140, 8, 6, 588, 3100, 2, 16, 230, 53, 2778, 7, 2, 2, 74, 91, 7, 27, 2, 587, 2, 10, 10, 21, 4, 667, 82, 272, 33, 85, 712, 40, 2, 5, 1736, 2, 4, 2, 2, 2, 1255, 1147, 1031, 2, 11, 4, 2, 2634, 29, 82, 1077, 4, 667, 8, 2, 998, 84, 2, 2335, 4, 564, 2992, 2, 7, 2, 2, 5, 60, 4, 2839, 2907, 2, 91, 7, 134, 757, 71, 828, 11, 14, 478, 2387, 201, 310, 48, 12, 9, 617, 174, 23, 6, 1877, 1709, 1277, 12]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_num = random.randint(0,25000)\n",
    "print(f'f_train at index {random_num}') # array of random words\n",
    "print(x_train[random_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The value of y_train assigns 0 to words with a negative connotation and 1 to words with a positive connotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train at index 5522\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(f'y_train at index {random_num}')\n",
    "print(y_train[random_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum allowance of words per IMDB review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First layer should be embedding because text is being analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "embedding_size = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(詞彙量, embedding_size, input_length=max_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 512, 32)           131072    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512, 32)           0         \n",
      "_________________________________________________________________\n",
      "unified_lstm_6 (UnifiedLSTM) (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 213,633\n",
      "Trainable params: 213,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "x_batch, y_batch = x_train[:batch_size], y_train[:batch_size]\n",
    "x_train2, y_train2 = x_train[batch_size:], y_train[batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24936 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "24936/24936 [==============================] - 200s 8ms/sample - loss: 0.5421 - accuracy: 0.7168 - val_loss: 0.4383 - val_accuracy: 0.8438\n",
      "Epoch 2/3\n",
      "24936/24936 [==============================] - 193s 8ms/sample - loss: 0.4037 - accuracy: 0.8199 - val_loss: 0.3075 - val_accuracy: 0.8750\n",
      "Epoch 3/3\n",
      "24936/24936 [==============================] - 192s 8ms/sample - loss: 0.3167 - accuracy: 0.8713 - val_loss: 0.1864 - val_accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb38ec8fd0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "model.fit(x_train2, y_train2, validation_data = (x_batch, y_batch),\n",
    "         batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a tweaked version of the above model, with a higher vocabulary base, lower dropout value, and more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "詞彙量 = 8192\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 詞彙量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 1024, 32)          262144    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "unified_lstm_7 (UnifiedLSTM) (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 344,705\n",
      "Trainable params: 344,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 32\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(詞彙量, embedding_size, input_length=max_words))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(128))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24936 samples, validate on 64 samples\n",
      "Epoch 1/10\n",
      "24936/24936 [==============================] - 499s 20ms/sample - loss: 0.4983 - accuracy: 0.7526 - val_loss: 0.3169 - val_accuracy: 0.8750\n",
      "Epoch 2/10\n",
      "24936/24936 [==============================] - 479s 19ms/sample - loss: 0.3116 - accuracy: 0.8744 - val_loss: 0.3641 - val_accuracy: 0.8594\n",
      "Epoch 3/10\n",
      "24936/24936 [==============================] - 452s 18ms/sample - loss: 0.2382 - accuracy: 0.9083 - val_loss: 0.3013 - val_accuracy: 0.8750\n",
      "Epoch 4/10\n",
      "24936/24936 [==============================] - 451s 18ms/sample - loss: 0.1982 - accuracy: 0.9253 - val_loss: 0.3495 - val_accuracy: 0.8594\n",
      "Epoch 5/10\n",
      "24936/24936 [==============================] - 446s 18ms/sample - loss: 0.1610 - accuracy: 0.9423 - val_loss: 0.3291 - val_accuracy: 0.8594\n",
      "Epoch 6/10\n",
      "24936/24936 [==============================] - 444s 18ms/sample - loss: 0.1450 - accuracy: 0.9481 - val_loss: 0.2305 - val_accuracy: 0.9219\n",
      "Epoch 7/10\n",
      "24936/24936 [==============================] - 442s 18ms/sample - loss: 0.1305 - accuracy: 0.9532 - val_loss: 0.2999 - val_accuracy: 0.9219\n",
      "Epoch 8/10\n",
      "24936/24936 [==============================] - 426s 17ms/sample - loss: 0.1127 - accuracy: 0.9613 - val_loss: 0.2805 - val_accuracy: 0.8906\n",
      "Epoch 9/10\n",
      "24936/24936 [==============================] - 427s 17ms/sample - loss: 0.0890 - accuracy: 0.9698 - val_loss: 0.3941 - val_accuracy: 0.8906\n",
      "Epoch 10/10\n",
      "24936/24936 [==============================] - 425s 17ms/sample - loss: 0.1299 - accuracy: 0.9501 - val_loss: 0.4434 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2cf48710>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "x_batch, y_batch = x_train[:batch_size], y_train[:batch_size]\n",
    "x_train2, y_train2 = x_train[batch_size:], y_train[batch_size:]\n",
    "\n",
    "num_epochs = 10\n",
    "model2.fit(x_train2, y_train2, validation_data = (x_batch, y_batch),\n",
    "         batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal of this recurrent neural network is to reach an accuracy rating of 87%. This accuracy seems to be reached after the second epoch. \n",
    "\n",
    "#### Some Cool Reference Material\n",
    "https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Coding with Pedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
